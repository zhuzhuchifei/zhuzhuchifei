import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv('XXX.csv') #insert your Excel
df = df.fillna({'WBC':29.2,'GR':12.0,'LY':6.51,'MCHC':201,'AST':29.2,'ALT':12.0,'TC':12.0,
                'Lp':12.0,'CRP':6.51,'LDH':201,'LDHD':167,'CK':57.25,'CKMB':12.0,})
df = df.fillna(value=df[["Hb","MCV","MCH","PLT","MPV","PCT","PDW","ALB","APO","HCY","LA"]].mean())
def Zscore(x):
    z=(x-np.mean(x))/np.std(x)
    return z
df.WBC=Zscore(df.WBC)
df.GR=Zscore(df.GR)
df.LY=Zscore(df.LY)
df.Hb=Zscore(df.Hb)
df.MCV=Zscore(df.MCV)
df.MCH=Zscore(df.MCH)
df.MCHC=Zscore(df.MCHC)
df.PLT=Zscore(df.PLT)
df.MPV=Zscore(df.MPV)
df.PCT=Zscore(df.PCT)
df.PDW=Zscore(df.PDW)
df.AST=Zscore(df.AST)
df.ALT=Zscore(df.ALT)
df.ALB=Zscore(df.ALB)
df.TC=Zscore(df.TC)
df.APO=Zscore(df.APO)
df.Lp=Zscore(df.Lp)
df.CRP=Zscore(df.CRP)
df.HCY=Zscore(df.HCY)
df.LA=Zscore(df.LA)
df.LDH=Zscore(df.LDH)
df.LDHD=Zscore(df.LDHD)
df.CK=Zscore(df.CK)
df.CKMB=Zscore(df.CKMB)
df.GJ=df.GJ.astype('category')
df.SZ=df.SZ.astype('category')
df = pd.get_dummies(df,drop_first=True)
cols_to_move = ['target']
remaining_cols = [col for col in df.columns if col not in cols_to_move]
df = df[remaining_cols + cols_to_move]
dataset = df.values
np.random.shuffle(dataset)
X = dataset[:,0:26].astype(float)
Y = dataset[:,26]
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
from sklearn.ensemble import RandomForestClassifier
clf_1 = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
clf_1.fit(X_train,Y_train)
clf_pred_1 = clf_1.predict(X_test)
from sklearn.metrics import accuracy_score
accuracy_1 = accuracy_score(Y_test, clf_pred_1)
print("模型准确率:", accuracy_1)
true_1 = np.sum(clf_pred_1 == Y_test )
print('预测对的结果数目为：', true_1)
false_1 =Y_test.shape[0]-true_1
print('预测错的的结果数目为：', false_1)
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
base_estimators = [
    DecisionTreeClassifier(max_depth=1),  
    SVC(kernel='linear', C=1.0), 
    RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=42) 
]
n_estimators = 100             
learning_rate = 0.5             
clf_2 = AdaBoostClassifier(estimator=base_estimators[2], n_estimators=n_estimators, learning_rate=learning_rate,algorithm='SAMME')
clf_2.fit(X_train, Y_train)
clf_pred_2 = clf_2.predict(X_test)
Y_pred_2 = clf_2.predict(X_test)
accuracy_2 = accuracy_score(Y_test, Y_pred_2)
print("Accuracy: %.2f%%" % (accuracy_2 * 100.0))
true_2 = np.sum(Y_pred_2 == Y_test )
print('预测对的结果数目为：', true_2)
false_2 =Y_test.shape[0]-true_2
print('预测错的的结果数目为：', false_2)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dropout
from tensorflow.keras import regularizers
from tensorflow.keras import optimizers
dataset = df.values
np.random.shuffle(dataset)
X = dataset[:,0:26].astype(float)
Y = to_categorical(dataset[:,26])
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
model = Sequential()
model.add(Dense(52,input_shape=(26,),activation='relu',
                kernel_regularizer=regularizers.l1(0.001),
                bias_regularizer=regularizers.l1(0.001)))
model.add(Dropout(0.50))
model.add(Dense(26,activation="relu",
                kernel_regularizer=regularizers.l1(0.001),
                bias_regularizer=regularizers.l1(0.001)))
model.add(Dropout(0.50))
model.add(Dense(13,activation="relu",
                kernel_regularizer=regularizers.l1(0.001),
                bias_regularizer=regularizers.l1(0.001)))
model.add(Dropout(0.50))
model.add(Dense(3,activation="softmax"))
METRICS = [ keras.metrics.BinaryAccuracy(name='accuracy')]
opt_adam = optimizers.Adam(learning_rate=0.001,decay=1e-4)
model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=METRICS)
history = model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=150,batch_size=6,verbose=1)
loss,accuracy = model.evaluate(X_train,Y_train)
print('训练数据集的准确度 = {:.2f}'.format(accuracy))
loss,accuracy = model.evaluate(X_test,Y_test)
print('测试数据集的准确度 = {:.2f}'.format(accuracy))
fig,loss_ax = plt.subplots()
acc_ax = loss_ax.twinx()
loss_ax.plot(history.history['loss'],'y',label='train loss')
loss_ax.plot(history.history['val_loss'],'r',label='test loss')
acc_ax.plot(history.history['accuracy'],'b',label='train accuracy')
acc_ax.plot(history.history['val_accuracy'],'g',label='test accuracy')
loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuracy')
loss_ax.legend(loc='upper left')
acc_ax.legend(loc='lower left')
plt.show()
Y_probs=model.predict(X_test)
from itertools import cycle
from sklearn.metrics import roc_curve, auc
lw = 2
n_classes = 3
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], Y_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
fpr["micro"], tpr["micro"], _ = roc_curve(Y_test.ravel(), Y_probs.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
plt.figure(1)
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)
plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))
plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Characteristic to multi-class')
plt.legend(loc="lower right")
plt.show()
import shap
shap.initjs()
explainer = shap.DeepExplainer(model,X_train)
shap_values = explainer.shap_values(X_test)
